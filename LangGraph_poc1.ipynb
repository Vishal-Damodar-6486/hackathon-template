{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LangGraph Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph,START,END \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "from typing import Dict, List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import JsonOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "load_dotenv()\n",
    "\n",
    "#gpt4O\n",
    "os.environ[\"AZURE_LLMsOpenAI_API_KEY\"] = os.getenv(\"AZURE_LLMsOpenAI_API_KEY\")\n",
    "os.environ[\"AZURE_LLMsOpenAI_ENDPOINT\"] = os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_LLMsOpenAI_API_VERSION\"] = os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\")\n",
    "os.environ[\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\"] = os.getenv(\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\")\n",
    "#Embeddings\n",
    "os.environ[\"AZURE_LLMsOpenAI_EMBEDDINGS_DEPLOYMENT_NAME\"] = os.getenv(\"AZURE_LLMsOpenAI_EMBEDDINGS_DEPLOYMENT_NAME\")\n",
    "#DOC AI\n",
    "os.environ[\"AZURE_DOC_AI_kEY\"] = os.getenv(\"AZURE_DOC_AI_kEY\")\n",
    "os.environ[\"AZURE_DOC_AI_PREVIEW\"] = os.getenv(\"AZURE_DOC_AI_PREVIEW\")\n",
    "os.environ[\"AZURE_DOC_AI_ENDPOINT\"] = os.getenv(\"AZURE_DOC_AI_ENDPOINT\")\n",
    "# gpt-4O\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key= os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"),\n",
    "    azure_deployment= os.getenv(\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint= os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=4096)\n",
    "llm.invoke(\"Hello\").content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(fileName):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=os.environ[\"AZURE_DOC_AI_ENDPOINT\"], credential=AzureKeyCredential(os.environ[\"AZURE_DOC_AI_kEY\"])\n",
    "    )\n",
    "\n",
    "    # Read the file as binary  \n",
    "    with open(fileName, \"rb\") as file:  \n",
    "        poller = document_analysis_client.begin_analyze_document(\"prebuilt-layout\", document=file)  \n",
    "        result = poller.result() \n",
    "\n",
    "    text=\"\"\"\"\"\"\n",
    "    total_pages=0\n",
    "    # Extract and print all the text from the document  \n",
    "    for page in result.pages:  \n",
    "        total_pages+=1\n",
    "        # print(f\"Page {page.page_number} has width: {page.width} and height: {page.height}, measured with unit: {page.unit}\")  \n",
    "        for line in page.lines:  \n",
    "            # print(line.content) \n",
    "            text+= line.content + \"\\n\"\n",
    "    return text, total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Graph State\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(BaseModel): \n",
    "    query: str = Field(default=\"\", description=\"User query input\") \n",
    "    flow_type: str = Field(default=None, description=\"Determines whether to perform comparison or Q&A\") \n",
    "    document_texts: List[str] = Field(default=None, description=\"List of document texts to process\") \n",
    "    comparison_results: Dict[str, List[str]] = Field(default=None, description=\"Stores commonalities and differences between documents\") \n",
    "    rag_answer: str = Field(default=None, description=\"Stores the RAG-based answer for Q&A flow\")\n",
    "\n",
    "def graph_state() -> GraphState: \n",
    "    return GraphState()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Documents\n",
    "\n",
    "document1 = \"This is a sample contract. The terms include payment within 30 days and a penalty clause.\" \n",
    "document2 = \"This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.\"\n",
    "\n",
    "document_texts = [document1, document2]\n",
    "\n",
    "# Load VectorDB for RAG\n",
    "def get_embeddings():\n",
    "    embeddings=AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"AZURE_LLMsOpenAI_EMBEDDINGS_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"))\n",
    "    return embeddings\n",
    "\n",
    "def create_vectordb(docs: List[str]): \n",
    "    embeddings = get_embeddings()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) \n",
    "    split_docs = text_splitter.create_documents(docs) \n",
    "    return FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "class router_class(BaseModel):\n",
    "    classifier: Literal[\"compare\",\"rag\",\"end\"]\n",
    "\n",
    "    \n",
    "def router(query: str):\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_key= os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "        openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"),\n",
    "        azure_deployment= os.getenv(\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\"),\n",
    "        azure_endpoint= os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=4096,\n",
    "        )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "            You are an AI assistant that classifies user queries into three categories:\n",
    "            - \"compare\" if the query involves comparing two documents.\n",
    "            - \"rag\" if the query requires question-answering with a retrieval-based solution.\n",
    "            - \"end\" if the user wants to quit/exit.\n",
    "\n",
    "            Return the response in **strict JSON format**, with **only one key**: classifier.\n",
    "            Do not include any explanations or extra text.\n",
    "            \"\"\"\n",
    "    router_prompt = ChatPromptTemplate([\n",
    "    (\"system\",system_prompt),\n",
    "    (\"user\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    # structured_llm= llm.with_structured_output(router_class)\n",
    "    parser = JsonOutputParser()\n",
    "    chain=router_prompt | llm |parser\n",
    "    response=chain.invoke({\"query\":query})\n",
    "    return response\n",
    "\n",
    "res=router(\"end\")\n",
    "print(res['classifier'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Node:Prompts user for Input Determines which flow to follow\n",
    "def decision_node(state: GraphState) -> GraphState: \n",
    "    query = input(\"Enter your query: \")\n",
    "    state=state.copy(update={\"query\":query})\n",
    "    print({state.query})\n",
    "    res = router(state.query)\n",
    "    flow_type=\"\"\n",
    "    print(res)\n",
    "    if \"compare\" in res['classifier']:\n",
    "        flow_type = \"comparison\"\n",
    "    elif \"end\" in res['classifier']:\n",
    "        flow_type = \"end\"\n",
    "    else:\n",
    "        flow_type = \"rag\"\n",
    "    state=state.copy(update={\"flow_type\":flow_type})\n",
    "    return state\n",
    "\n",
    "\n",
    "# Comparison Node: Compares full document texts\n",
    "\n",
    "def comparison_node(state: GraphState) -> GraphState: \n",
    "    doc1, doc2 = state.document_texts \n",
    "    common_text = set(doc1.split()) & set(doc2.split()) \n",
    "    differences = set(doc1.split()) ^ set(doc2.split()) \n",
    "    return state.copy(update={\"comparison_results\": {\"common\": list(common_text), \"differences\": list(differences)}})\n",
    "\n",
    "# Q&A Node: Uses RAG with VectorDB\n",
    "\n",
    "def qa_node(state: GraphState) -> GraphState: \n",
    "    vectordb = create_vectordb(state.document_texts) \n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\") \n",
    "    chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), memory=memory) \n",
    "    response = chain.run(state.query) \n",
    "    return state.copy(update={\"rag_answer\": response})\n",
    "\n",
    "def end_node(state: GraphState) -> GraphState:\n",
    "    print(\"workflow has ended\")\n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x21b99b49ff0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(GraphState)\n",
    "\n",
    "graph.add_node(\"decision_node\", decision_node) \n",
    "graph.add_node(\"comparison_node\", comparison_node) \n",
    "graph.add_node(\"qa_node\", qa_node) \n",
    "graph.add_node(\"end_node\", end_node) \n",
    "\n",
    "graph.add_edge(\"comparison_node\",\"decision_node\") # Return to decision node after comparison\n",
    "graph.add_edge(\"qa_node\",\"decision_node\") # Return to decision node after Q&A\n",
    "\n",
    "# Define Conditional Edges\n",
    "def route_fn(state:GraphState)->str:\n",
    "    return state.flow_type\n",
    "    \n",
    "graph.add_conditional_edges(\n",
    "    \"decision_node\",\n",
    "    route_fn,\n",
    "    {\n",
    "       \"comparison\":\"comparison_node\",\n",
    "       \"rag\":\"qa_node\",\n",
    "       \"end\":\"end_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.set_entry_point(\"decision_node\") \n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='mermaid.ink', port=443): Max retries exceeded with url: /img/JSV7aW5pdDogeydmbG93Y2hhcnQnOiB7J2N1cnZlJzogJ2xpbmVhcid9fX0lJQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA+X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCWRlY2lzaW9uX25vZGUoZGVjaXNpb25fbm9kZSkKCWNvbXBhcmlzb25fbm9kZShjb21wYXJpc29uX25vZGUpCglxYV9ub2RlKHFhX25vZGUpCgllbmRfbm9kZShbZW5kX25vZGVdKTo6Omxhc3QKCV9fc3RhcnRfXyAtLT4gZGVjaXNpb25fbm9kZTsKCWNvbXBhcmlzb25fbm9kZSAtLT4gZGVjaXNpb25fbm9kZTsKCXFhX25vZGUgLS0+IGRlY2lzaW9uX25vZGU7CglkZWNpc2lvbl9ub2RlIC0uICZuYnNwO2NvbXBhcmlzb24mbmJzcDsgLi0+IGNvbXBhcmlzb25fbm9kZTsKCWRlY2lzaW9uX25vZGUgLS4gJm5ic3A7cmFnJm5ic3A7IC4tPiBxYV9ub2RlOwoJZGVjaXNpb25fbm9kZSAtLiAmbmJzcDtlbmQmbmJzcDsgLi0+IGVuZF9ub2RlOwoJY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNmMmYwZmYsbGluZS1oZWlnaHQ6MS4yCgljbGFzc0RlZiBmaXJzdCBmaWxsLW9wYWNpdHk6MAoJY2xhc3NEZWYgbGFzdCBmaWxsOiNiZmI2ZmMK?type=png&bgColor=!white (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(workflow.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:4: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"query\":query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi'}\n",
      "{'classifier': 'rag'}\n",
      "decision_node {'query': 'hi', 'flow_type': 'rag', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:15: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"flow_type\":flow_type})\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:31: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(state.query)\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:34: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  return state.copy(update={\"rag_answer\": response})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_node {'query': 'hi', 'flow_type': 'rag', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:4: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"query\":query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end'}\n",
      "{'classifier': 'end'}\n",
      "decision_node {'query': 'end', 'flow_type': 'end', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n",
      "workflow has ended\n",
      "end_node {'query': 'end', 'flow_type': 'end', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:15: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"flow_type\":flow_type})\n"
     ]
    }
   ],
   "source": [
    "initial_state = GraphState(document_texts=document_texts) \n",
    "# workflow.invoke(initial_state)\n",
    "\n",
    "for event in workflow.stream(initial_state):\n",
    "    for key,value in event.items():\n",
    "        print(key,value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
